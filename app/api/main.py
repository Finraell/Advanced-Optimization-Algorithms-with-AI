"""Entry point for the API service.

This FastAPI application defines a handful of endpoints that demonstrate how the
optimisation platform might be shaped.  The handlers are intentionally
minimal – they return static responses to illustrate request/response
schemas.  In a full implementation these would call into the modelling layer,
persist model versions, submit runs to a task queue and return results.
"""

from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel, Field
from typing import Any, Dict, Optional

from sqlalchemy.orm import Session

from .database import get_db
from . import models

# Import authentication utilities and role enforcement.  The auth
# router defines OAuth/OIDC login endpoints and dependencies for
# retrieving the current user and checking permissions.  If the auth
# module is unavailable (e.g. during early scaffolding), these
# imports will fall back to ``None`` so that the API can still run.
try:
    from .auth import router as auth_router, require_role  # type: ignore
except Exception:
    auth_router = None  # type: ignore
    def require_role(*args, **kwargs):  # type: ignore
        def dummy_dependency():
            return None
        return dummy_dependency

# Import the AI translation provider from the ai package.  This provider
# encapsulates the logic for calling a large language model and parsing
# the returned JSON into a model schema.  At runtime the API key can be
# provided via the OPENAI_API_KEY environment variable.
try:
    from ..ai import LLMProvider, TranslationRequest as LLMTranslationRequest  # type: ignore
except Exception:
    # Fallback when ai package is not available (e.g. during initial scaffolding).
    LLMProvider = None  # type: ignore
    LLMTranslationRequest = None  # type: ignore


app = FastAPI(title="Advanced Optimization Algorithms with AI",
              description="APIs for translating, storing and running optimisation models.")

# Include authentication routes if available
if auth_router is not None:
    app.include_router(auth_router)


class TranslateRequest(BaseModel):
    prompt: str = Field(..., description="Natural language description of the optimisation problem.")
    domain: Optional[str] = Field(None, description="Optional domain hint (e.g. supply_chain, scheduling).")
    output_format: str = Field("json", description="Output format for the generated model (json or pyomo).")


class TranslateResponse(BaseModel):
    model_json: Dict[str, Any]
    pyomo_script: Optional[str] = None
    audits: Dict[str, Any] = Field(default_factory=dict)
    recommendations: Dict[str, Any] = Field(default_factory=dict)


@app.post(
    "/v1/models/translate",
    response_model=TranslateResponse,
    dependencies=[Depends(require_role(["admin", "editor"]))],
)
async def translate_model(req: TranslateRequest) -> TranslateResponse:
    """Translate a natural language prompt into a model schema.

    This implementation is a stub: it returns a trivial linear programme for
    demonstration purposes.  In a production system this would call an AI
    service (OpenAI/vLLM) and a rule based parser to construct a valid
    optimisation model.
    """
    """Translate a natural language problem into a formal model.

    This implementation delegates translation to a large language model via
    the ``LLMProvider`` class.  The provider performs automatic retries and
    basic validation.  If the provider is not configured or the translation
    fails, a HTTP 500 error will be returned.  The response includes the
    generated model JSON and a placeholder Pyomo script.
    """
    if LLMProvider is None or LLMTranslationRequest is None:
        raise HTTPException(status_code=500, detail="AI translation service is not available")

    provider = LLMProvider()
    try:
        llm_req = LLMTranslationRequest(prompt=req.prompt, domain=req.domain, output_format=req.output_format)
        model_json = provider.translate_to_model(llm_req)
    except RuntimeError as exc:
        raise HTTPException(status_code=500, detail=str(exc))

    return TranslateResponse(
        model_json=model_json,
        pyomo_script="# Pyomo script would be generated by a separate service",
        audits={"status": "ok"},
        recommendations={"solver": "auto", "time_limit_sec": 300}
    )


class RunRequest(BaseModel):
    model_version_id: str
    solver: str
    parameters: Dict[str, Any] = Field(default_factory=dict)


class RunResponse(BaseModel):
    run_id: str
    status: str
    objective_value: Optional[float] = None


@app.post(
    "/v1/runs",
    response_model=RunResponse,
    dependencies=[Depends(require_role(["admin", "editor"]))],
)
async def start_run(req: RunRequest, db: Session = Depends(get_db)) -> RunResponse:
    """Submit a model run request.

    Create a new ``Run`` record in the database with status ``pending`` and
    enqueue the job to be executed by a Celery worker.  A unique run
    identifier is generated using the model version identifier and a
    timestamp.  In a production system this would use a UUID or
    ULID generator.
    """
    import time
    # Create a simple deterministic run_id; in production use uuid.uuid4()
    run_id = f"run_{int(time.time())}"

    # Persist run metadata
    run = models.Run(
        model_version_id=int(req.model_version_id),
        solver=req.solver,
        parameters_json=req.parameters,
        status="pending",
        seed=req.parameters.get("seed"),
        time_limit_sec=req.parameters.get("time_limit_sec"),
        created_at=time.strftime("%Y-%m-%d %H:%M:%S"),
        updated_at=time.strftime("%Y-%m-%d %H:%M:%S"),
    )
    db.add(run)
    db.commit()
    db.refresh(run)

    # Enqueue the solve task via Celery.  Import locally to avoid circular import.
    try:
        from ..workers.worker import solve_model_task  # type: ignore
        solve_model_task.delay(run_id=run_id, model_json=req.parameters.get("model_json", {}), solver=req.solver, params=req.parameters)
    except Exception:
        # If Celery is not configured or import fails, leave the run pending
        pass

    return RunResponse(run_id=run_id, status="pending")


@app.get(
    "/v1/runs/{run_id}",
    response_model=RunResponse,
    dependencies=[Depends(require_role(["admin", "editor", "viewer"]))],
)
async def get_run(run_id: str, db: Session = Depends(get_db)) -> RunResponse:
    """Retrieve the status of a run.

    Query the database for the run record by identifier and return its
    status and objective value.  If the run does not exist, return 404.
    """
    run = db.query(models.Run).filter(models.Run.id == run_id).first()
    if not run:
        raise HTTPException(status_code=404, detail="Run not found")
    return RunResponse(run_id=str(run.id), status=run.status, objective_value=run.objective_value)
